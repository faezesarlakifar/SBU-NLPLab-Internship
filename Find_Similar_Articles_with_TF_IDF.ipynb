{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/faezesarlakifar/SBU-NLPLab-Internship/blob/main/Find_Similar_Articles_with_TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efd6a568",
      "metadata": {
        "id": "efd6a568"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.cluster import MiniBatchKMeans "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the articles "
      ],
      "metadata": {
        "id": "10Gd4qZUoyNt"
      },
      "id": "10Gd4qZUoyNt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94c24e3e",
      "metadata": {
        "id": "94c24e3e"
      },
      "outputs": [],
      "source": [
        "X = pd.read_csv(r\"../atricles/all-acl-articles-with-content.csv\") \n",
        "X = X.dropna(thresh=3)  \n",
        "X = X.dropna(subset = ['Content']) \n",
        "X = X.reset_index(drop=True) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preview data"
      ],
      "metadata": {
        "id": "OOJJSo6go50w"
      },
      "id": "OOJJSo6go50w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b495ebd1",
      "metadata": {
        "id": "b495ebd1",
        "outputId": "1c454cfb-920e-4aeb-e1c7-bc49d35c509d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>URL</th>\n",
              "      <th>PDF_URL</th>\n",
              "      <th>Authors</th>\n",
              "      <th>Citation</th>\n",
              "      <th>Year of Publish</th>\n",
              "      <th>Content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17th Annual Meeting of the Association for Com...</td>\n",
              "      <td>https://aclanthology.org/P79-1000/</td>\n",
              "      <td>https://aclanthology.org/P79-1000.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1979.0</td>\n",
              "      <td>b'                                   Front Mat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Syntactic Processing</td>\n",
              "      <td>https://aclanthology.org/P79-1001/</td>\n",
              "      <td>https://aclanthology.org/P79-1001.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>73.0</td>\n",
              "      <td>1979.0</td>\n",
              "      <td>b'                                        Synt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Towards a Self-Extending Parser</td>\n",
              "      <td>https://aclanthology.org/P79-1002/</td>\n",
              "      <td>https://aclanthology.org/P79-1002.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>72.0</td>\n",
              "      <td>1979.0</td>\n",
              "      <td>b'                                        TOWA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Word Expert Parsing</td>\n",
              "      <td>https://aclanthology.org/P79-1003/</td>\n",
              "      <td>https://aclanthology.org/P79-1003.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1979.0</td>\n",
              "      <td>b'                                        WORD...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Toward a Computational Theory of Speech Percep...</td>\n",
              "      <td>https://aclanthology.org/P79-1005/</td>\n",
              "      <td>https://aclanthology.org/P79-1005.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1979.0</td>\n",
              "      <td>b'                                        TOWA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>Unsupervised Language Model Adaptation Incorpo...</td>\n",
              "      <td>https://aclanthology.org/P07-1085/</td>\n",
              "      <td>https://aclanthology.org/P07-1085.pdf</td>\n",
              "      <td>Yang Liu</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>b'                                            ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>Coordinate Noun Phrase Disambiguation in a Gen...</td>\n",
              "      <td>https://aclanthology.org/P07-1086/</td>\n",
              "      <td>https://aclanthology.org/P07-1086.pdf</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>b'                                            ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>A Unified Tagging Approach to Text Normalization</td>\n",
              "      <td>https://aclanthology.org/P07-1087/</td>\n",
              "      <td>https://aclanthology.org/P07-1087.pdf</td>\n",
              "      <td>Jie Tang, Hang Li, Hwee Tou Ng, Tiejun Zhao</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>b'                                            ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>Forest-to-String Statistical Translation Rules</td>\n",
              "      <td>https://aclanthology.org/P07-1089/</td>\n",
              "      <td>https://aclanthology.org/P07-1089.pdf</td>\n",
              "      <td>Yun Huang, Qun Liu, Shouxun Lin</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>b'                                            ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>Ordering Phrases with Function Words</td>\n",
              "      <td>https://aclanthology.org/P07-1090/</td>\n",
              "      <td>https://aclanthology.org/P07-1090.pdf</td>\n",
              "      <td>Min-Yen Kan, Haizhou Li</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>b'                                            ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows Ã— 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Title  \\\n",
              "0     17th Annual Meeting of the Association for Com...   \n",
              "1                                  Syntactic Processing   \n",
              "2                       Towards a Self-Extending Parser   \n",
              "3                                   Word Expert Parsing   \n",
              "4     Toward a Computational Theory of Speech Percep...   \n",
              "...                                                 ...   \n",
              "1995  Unsupervised Language Model Adaptation Incorpo...   \n",
              "1996  Coordinate Noun Phrase Disambiguation in a Gen...   \n",
              "1997   A Unified Tagging Approach to Text Normalization   \n",
              "1998     Forest-to-String Statistical Translation Rules   \n",
              "1999               Ordering Phrases with Function Words   \n",
              "\n",
              "                                     URL  \\\n",
              "0     https://aclanthology.org/P79-1000/   \n",
              "1     https://aclanthology.org/P79-1001/   \n",
              "2     https://aclanthology.org/P79-1002/   \n",
              "3     https://aclanthology.org/P79-1003/   \n",
              "4     https://aclanthology.org/P79-1005/   \n",
              "...                                  ...   \n",
              "1995  https://aclanthology.org/P07-1085/   \n",
              "1996  https://aclanthology.org/P07-1086/   \n",
              "1997  https://aclanthology.org/P07-1087/   \n",
              "1998  https://aclanthology.org/P07-1089/   \n",
              "1999  https://aclanthology.org/P07-1090/   \n",
              "\n",
              "                                    PDF_URL  \\\n",
              "0     https://aclanthology.org/P79-1000.pdf   \n",
              "1     https://aclanthology.org/P79-1001.pdf   \n",
              "2     https://aclanthology.org/P79-1002.pdf   \n",
              "3     https://aclanthology.org/P79-1003.pdf   \n",
              "4     https://aclanthology.org/P79-1005.pdf   \n",
              "...                                     ...   \n",
              "1995  https://aclanthology.org/P07-1085.pdf   \n",
              "1996  https://aclanthology.org/P07-1086.pdf   \n",
              "1997  https://aclanthology.org/P07-1087.pdf   \n",
              "1998  https://aclanthology.org/P07-1089.pdf   \n",
              "1999  https://aclanthology.org/P07-1090.pdf   \n",
              "\n",
              "                                          Authors  Citation  Year of Publish  \\\n",
              "0                                             NaN       0.0           1979.0   \n",
              "1                                             NaN      73.0           1979.0   \n",
              "2                                             NaN      72.0           1979.0   \n",
              "3                                             NaN      13.0           1979.0   \n",
              "4                                             NaN       0.0           1979.0   \n",
              "...                                           ...       ...              ...   \n",
              "1995                                     Yang Liu       0.0           2007.0   \n",
              "1996                                          NaN       0.0           2007.0   \n",
              "1997  Jie Tang, Hang Li, Hwee Tou Ng, Tiejun Zhao       0.0           2007.0   \n",
              "1998              Yun Huang, Qun Liu, Shouxun Lin       0.0           2007.0   \n",
              "1999                      Min-Yen Kan, Haizhou Li       0.0           2007.0   \n",
              "\n",
              "                                                Content  \n",
              "0     b'                                   Front Mat...  \n",
              "1     b'                                        Synt...  \n",
              "2     b'                                        TOWA...  \n",
              "3     b'                                        WORD...  \n",
              "4     b'                                        TOWA...  \n",
              "...                                                 ...  \n",
              "1995  b'                                            ...  \n",
              "1996  b'                                            ...  \n",
              "1997  b'                                            ...  \n",
              "1998  b'                                            ...  \n",
              "1999  b'                                            ...  \n",
              "\n",
              "[2000 rows x 7 columns]"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.head(2000) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data information"
      ],
      "metadata": {
        "id": "4g1tIZlCpElF"
      },
      "id": "4g1tIZlCpElF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ec55b7f",
      "metadata": {
        "id": "7ec55b7f",
        "outputId": "0914b616-c48d-47ae-fe8d-fde1a8b48aec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8747 entries, 0 to 8746\n",
            "Data columns (total 7 columns):\n",
            " #   Column           Non-Null Count  Dtype  \n",
            "---  ------           --------------  -----  \n",
            " 0   Title            8747 non-null   object \n",
            " 1   URL              8747 non-null   object \n",
            " 2   PDF_URL          8747 non-null   object \n",
            " 3   Authors          7597 non-null   object \n",
            " 4   Citation         8747 non-null   float64\n",
            " 5   Year of Publish  8747 non-null   float64\n",
            " 6   Content          8747 non-null   object \n",
            "dtypes: float64(2), object(5)\n",
            "memory usage: 478.5+ KB\n"
          ]
        }
      ],
      "source": [
        "X.info() "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF calculation"
      ],
      "metadata": {
        "id": "rB1wYWjkpO6L"
      },
      "id": "rB1wYWjkpO6L"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b11051fd",
      "metadata": {
        "id": "b11051fd"
      },
      "outputs": [],
      "source": [
        "text_content = X['Content'] \n",
        "vector = TfidfVectorizer(max_df=0.2,         # drop words that occur in more than X percent of documents\n",
        "                             #min_df=8,      # only use words that appear at least X times\n",
        "                             stop_words='english', \n",
        "                             lowercase=True, \n",
        "                             use_idf=True,   \n",
        "                             norm=u'l2',     \n",
        "                             smooth_idf=True \n",
        "                            )\n",
        "tfidf = vector.fit_transform(text_content) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "776aef81",
      "metadata": {
        "id": "776aef81"
      },
      "outputs": [],
      "source": [
        "def search(tfidf_matrix,model,request, top_n = 5):\n",
        "    request_transform = model.transform([request])\n",
        "    similarity = np.dot(request_transform,np.transpose(tfidf_matrix))\n",
        "    x = np.array(similarity.toarray()[0])\n",
        "    indices=np.argsort(x)[-5:][::-1]\n",
        "    return indices \n",
        "\n",
        "def find_similar(tfidf_matrix, index, top_n = 5):\n",
        "    cosine_similarities = linear_kernel(tfidf_matrix[index:index+1], tfidf_matrix).flatten()\n",
        "    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n",
        "    return [index for index in related_docs_indices][0:top_n]    \n",
        " \n",
        "def print_result(request_content,indices,X):\n",
        "    print('\\nsearch : ' + request_content)\n",
        "    print('\\nBest Results :') \n",
        "    for i in indices:\n",
        "        print('id = {0:5d} - title = {1}'.format(i,X['Title'].loc[i])) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9dacd11",
      "metadata": {
        "id": "f9dacd11",
        "outputId": "3e5342c9-2c49-4ae3-da92-7020f6426c25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "search : Ordering Phrases with Function Words\n",
            "\n",
            "Best Results :\n",
            "id =  3619 - title = Dependency-based Pre-ordering for Chinese-English Machine Translation\n",
            "id =  2926 - title = Sentence Ordering Driven by Local and Global Coherence for Summary Generation\n",
            "id =  1680 - title = A Bottom-Up Approach to Sentence Ordering for Multi-Document Summarization\n",
            "id =  2851 - title = Semi-Supervised Modeling for Prenominal Modifier Ordering\n",
            "id =  3316 - title = Learning to Order Natural Language Texts\n"
          ]
        }
      ],
      "source": [
        "request = 'Ordering Phrases with Function Words'\n",
        "\n",
        "result = search(tfidf,vector, request, top_n = 5)\n",
        "print_result(request,result,X) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c6d8d5",
      "metadata": {
        "id": "10c6d8d5",
        "outputId": "06927614-c90a-46cc-d79c-436f6fc75a7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'                               A Noisy-Channel Model for Document Compression  Hal Daume\\xcc\\x81 III and Daniel Marcu Information Sciences Institute  University of Southern California 4676 Admiralty Way  Suite 1001  Marina del Rey  CA 90292\\xef\\xbf\\xbd hdaume marcu \\xef\\xbf\\xbd @isi.edu  Abstract  We present a document compression sys- tem that uses a hierarchical noisy-channel model of text production. Our compres- sion system first automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input. The system then uses a sta- tistical hierarchical model of text produc- tion in order to drop non-important syn- tactic and discourse constituents so as to generate coherent  grammatical document compressions of arbitrary length. The sys- tem outperforms both a baseline and a sentence-based compression system that operates by simplifying sequentially all sentences in a text. Our results support the claim that discourse knowledge plays an important role in document summariza- tion.  1 Introduction Single document summarization systems proposed to date fall within one of the following three classes:  Extractive summarizers simply select and present to the user the most important sentences in a text \\xe2\\x80\\x94 see (Mani and Maybury  1999; Marcu  2000; Mani  2001) for comprehensive overviews of the methods and algorithms used to accomplish this.  Headline generators are noisy-channel probabilis- tic systems that are trained on large corpora of \\xef\\xbf\\xbd Headline  Text \\xef\\xbf\\xbd pairs (Banko et al.  2000;  Berger and Mittal  2000). These systems pro- duce short sequences of words that are indica- tive of the content of the text given as input.  Sentence simplification systems (Chandrasekar et al.  1996; Mahesh  1997; Carroll et al.  1998; Grefenstette  1998; Jing  2000; Knight and Marcu  2000) are capable of compressing long sentences by deleting unimportant words and phrases.  Extraction-based summarizers often produce out- puts that contain non-important sentence fragments. For example  the hypothetical extractive summary of Text (1)  which is shown in Table 1  can be com- pacted further by deleting the clause \\xe2\\x80\\x9cwhich is al- ready almost enough to win\\xe2\\x80\\x9d. Headline-based sum- maries  such as that shown in Table 1  are usually indicative of a text\\xe2\\x80\\x99s content but not informative  grammatical  or coherent. By repeatedly applying a sentence-simplification algorithm one sentence at a time  one can compress a text; yet  the outputs gen- erated in this way are likely to be incoherent and to contain unimportant information. When summa- rizing text  some sentences should be dropped alto- gether.  Ideally  we would like to build systems that have the strengths of all these three classes of approaches. The \\xe2\\x80\\x9cDocument Compression\\xe2\\x80\\x9d entry in Table 1 shows a grammatical  coherent summary of Text (1)  which was generated by a hypothetical document compression system that preserves the most impor- tant information in a text while deleting sentences  phrases  and words that are subsidiary to the main message of the text. Obviously  generating coher- ent  grammatical summaries such as that produced by the hypothetical document compression system in Table 1 is not trivial because of many conflicting                  Computational Linguistics (ACL)  Philadelphia  July 2002  pp. 449-456.                          Proceedings of the 40th Annual Meeting of the Association for    Type of Hypothetical output Output Output is Output is Summarizer contains only coherent grammatical  important info Extractive John Doe has already secured the vote of most \\xef\\xbf\\xbd summarizer democrats in his constituency  which is already  almost enough to win. But without the support of the governer  he is still on shaky ground.  Headline mayor vote constituency governer \\xef\\xbf\\xbd generator Sentence The mayor is now looking for re-election. John Doe \\xef\\xbf\\xbd simplifier has already secured the vote of most democrats  in his constituency. He is still on shaky ground. Document John Doe has secured the vote of most democrats. \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd compressor But he is still on shaky ground.  Table 1: Hypothetical outputs generated by various types of summarizers.  goals1. The deletion of certain sentences may result in incoherence and information loss. The deletion of certain words and phrases may also lead to ungram- maticality and information loss.  The mayor is now looking for re-election. John Doe  has already secured the vote of most democrats in his  constituency  which is already almost enough to win.  But without the support of the governer  he is still on  shaky grounds.  (1)  In this paper  we present a document compression system that uses hierarchical models of discourse and syntax in order to simultaneously manage all these conflicting goals. Our compression system first automatically derives the syntactic structure of each sentence and the overall discourse structure of the text given as input. The system then uses a sta- tistical hierarchical model of text production in or- der to drop non-important syntactic and discourse units so as to generate coherent  grammatical doc- ument compressions of arbitrary length. The system outperforms both a baseline and a sentence-based compression system that operates by simplifying se- quentially all sentences in a text.  2 Document Compression The document compression task is conceptually simple. Given a document \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\t\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\r\\t\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\t\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd   our goal is to produce a new document \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd by \\xe2\\x80\\x9cdropping\\xe2\\x80\\x9d words \\t\\xef\\xbf\\xbd\\xef\\xbf\\xbd from \\xef\\xbf\\xbd . In order to achieve this goal  we  1A number of other systems use the outputs of extrac- tive summarizers and repair them to improve coherence (DUC  2001; DUC  2002). Unfortunately  none of these seems flexible enough to produce in one shot good summaries that are simul- taneously coherent and grammatical.  extent the noisy-channel model proposed by Knight & Marcu (2000). Their system compressed sen- tences by dropping syntactic constituents  but could be applied to entire documents only on a sentence- by-sentence basis. As discussed in Section 1  this is not adequate because the resulting summary may contain many compressed sentences that are irrele- vant. In order to extend Knight & Marcu\\xe2\\x80\\x99s approach beyond the sentence level  we need to \\xe2\\x80\\x9cglue\\xe2\\x80\\x9d sen- tences together in a tree structure similar to that used at the sentence level. Rhetorical Structure Theory (RST) (Mann and Thompson  1988) provides us this \\xe2\\x80\\x9cglue.\\xe2\\x80\\x9d  The tree in Figure 1 depicts the RST structure of Text (1). In RST  discourse structures are non- binary trees whose leaves correspond to elementary discourse units (EDUs)  and whose internal nodes correspond to contiguous text spans. Each internal node in an RST tree is characterized by a rhetor- ical relation. For example  the first sentence in Text (1) provides BACKGROUND information for inter- preting the information in sentences 2 and 3  which are in a CONTRAST relation (see Figure 1). Each re- lation holds between two adjacent non-overlapping text spans called NUCLEUS and SATELLITE. (There are a few exceptions to this rule: some relations  such as LIST and CONTRAST  are multinuclear.) The dis- tinction between nuclei and satellites comes from the empirical observation that the nucleus expresses what is more essential to the writer\\xe2\\x80\\x99s purpose than the satellite.  Our system is able to analyze both the discourse structure of a document and the syntactic structure of each of its sentences or EDUs. It then compresses    the document by dropping either syntactic or dis- course constituents.  3 A Noisy-Channel Model For a given document \\xef\\xbf\\xbd   we want to find the summary text \\xef\\xbf\\xbd that maximizes \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd!\\xef\\xbf\\xbd#\" . Using Bayes rule  we flip this so we end up maximizing \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd$\\xef\\xbf\\xbd%\\xef\\xbf\\xbd&\"\\'\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd&\" . Thus  we are left with modelling two probability distributions: \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd$\\xef\\xbf\\xbd%\\xef\\xbf\\xbd&\"   the probability of a document \\xef\\xbf\\xbd given a summary \\xef\\xbf\\xbd   and \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd(\"   the probability of a summary. We assume that we are given the discourse structure of each document and the syntactic structures of each of its EDUs.  The intuitive way of thinking about this applica- tion of Bayes rule  reffered to as the noisy-channel model  is that we start with a summary \\xef\\xbf\\xbd and add \\xe2\\x80\\x9cnoise\\xe2\\x80\\x9d to it  yielding a longer document \\xef\\xbf\\xbd . The noise added in our model consists of words  phrases and discourse units.  For instance  given the document \\xe2\\x80\\x9cJohn Doe has secured the vote of most democrats.\\xe2\\x80\\x9d we could add words to it (namely the word \\xe2\\x80\\x9calready\\xe2\\x80\\x9d) to gener- ate \\xe2\\x80\\x9cJohn Doe has already secured the vote of most democrats.\\xe2\\x80\\x9d We could also choose to add an en- tire syntactic constituent  for instance a prepositional phrase  to generate \\xe2\\x80\\x9cJohn Doe has secured the vote of most democrats in his constituency.\\xe2\\x80\\x9d These are both examples of sentence expansion as used previ- ously by Knight & Marcu (2000).  Our system  however  also has the ability to ex- pand on a core message by adding discourse con- stituents. For instance  it could decide to add another discourse constituent to the original summary \\xe2\\x80\\x9cJohn Doe has secured the vote of most democrats\\xe2\\x80\\x9d by CONTRASTing the information in the summary with the uncertainty regarding the support of the gover- nor  thus yielding the text: \\xe2\\x80\\x9cJohn Doe has secured the vote of most democrats. But without the support of the governor  he is still on shaky ground.\\xe2\\x80\\x9d  As in any noisy-channel application  there are three parts that we have to account for if we are to build a complete document compression system: the channel model  the source model and the decoder. We describe each of these below.  The source model assigns to a string the probabil- ity \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd(\"   the probability that the summary \\xef\\xbf\\xbd is good English. Ideally  the source model should disfavor ungrammatical sentences and  documents containing incoherently juxtaposed sentences.  The channel model assigns to any docu- ment/summary pair a probability \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd)\\xef\\xbf\\xbd*\\xef\\xbf\\xbd%\\xef\\xbf\\xbd(\" . This models the extent to which \\xef\\xbf\\xbd is a good expansion of \\xef\\xbf\\xbd . For instance  if \\xef\\xbf\\xbd is \\xe2\\x80\\x9cThe mayor is now looking for re-election.\\xe2\\x80\\x9d  \\xef\\xbf\\xbd+\\xef\\xbf\\xbd is \\xe2\\x80\\x9cThe mayor is now looking for re-election. He has to secure the vote of the democrats.\\xe2\\x80\\x9d and \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd is \\xe2\\x80\\x9cThe major is now looking for re-election. Sharks have sharp teeth.\\xe2\\x80\\x9d  we expect \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd-\\xef\\xbf\\xbd%\\xef\\xbf\\xbd(\" to be higher than \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd.\\xef\\xbf\\xbd%\\xef\\xbf\\xbd&\" because \\xef\\xbf\\xbd \\xef\\xbf\\xbd expands on \\xef\\xbf\\xbd by elaboration  while \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd shifts to a different topic  yielding an incoherent text.  The decoder searches through all possible sum- maries of a document \\xef\\xbf\\xbd for the summary \\xef\\xbf\\xbd that maximizes the posterior probability \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd)\\xef\\xbf\\xbd*\\xef\\xbf\\xbd%\\xef\\xbf\\xbd(\"\\r\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd(\" .  Each of these parts is described below.  3.1 Source model  The job of the source model is to assign a score \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd(\" to a compression independent of the original document. That is  the source model should measure how good English a summary is (independent of whether it is a good compression or not). Currently  we use a bigram measure of quality (trigram scores were also tested but failed to make a difference)  combined with non-lexicalized context-free syntac- tic probabilities and context-free discourse probabil- ities  giving \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd(\"/\\xef\\xbf\\xbd \\xef\\xbf\\xbd102\\xef\\xbf\\xbd436587\\'9\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd&\"\\xef\\xbf\\xbd:\\xef\\xbf\\xbd\\xef\\xbf\\xbd<;>=@?\\xef\\xbf\\xbdA\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd(\"\\xef\\xbf\\xbd: \\xef\\xbf\\xbd<BC;>=@?\\xef\\xbf\\xbdA \\xef\\xbf\\xbd)\\xef\\xbf\\xbd&\" . It would be better to use a lexical- ized context free grammar  but that was not possible given the decoder used.  3.2 Channel model  The channel model is allowed to add syntactic constituents (through a stochastic operation called constituent-expand) or discourse units (through an- other stochastic operation called EDU-expand). Both of these operations are performed on a com- bined discourse/syntax tree called the DS-tree. The DS-tree for Text (1) is shown in Figure 1 for refer- ence.  Suppose we start with the summary \\xef\\xbf\\xbdD\\xef\\xbf\\xbd \\xe2\\x80\\x9cThe mayor is looking for re-election.\\xe2\\x80\\x9d A constituent-    E F F G  S  NPB  DT NN  VP  VBZ ADVP  RB  VP\\xe2\\x88\\x92A  VBG PP  NPB  NN PUNC.  IN  The mayor  now looking  for  is  re\\xe2\\x88\\x92election .  H I G J K I L M N O F P Q R TOP  John Doe has already secured the vote of most democrats in his constituency   which is already almost enough to win.  But without the support of the governer   he is still on shaky ground.  S P L J H T I Q H I G J U V I W P I G X F Q H I G J L F Q R X G X F Q S P L J H T I Q S P L J Y F Q G O I Z G S P L J Y F Q G O I Z G  S P L J H T I Q  * *  Figure 1: The discourse (full)/syntax (partial) tree for Text (1).  expand operation could insert a syntactic con- stituent  such as \\xe2\\x80\\x9cthis year\\xe2\\x80\\x9d anywhere in the syntac- tic tree of \\xef\\xbf\\xbd . A constituent-expand operation could also add single words: for instance the word \\xe2\\x80\\x9cnow\\xe2\\x80\\x9d could be added between \\xe2\\x80\\x9cis\\xe2\\x80\\x9d and \\xe2\\x80\\x9clooking \\xe2\\x80\\x9d yielding \\xef\\xbf\\xbd[\\xef\\xbf\\xbd \\xe2\\x80\\x9cThe mayor is now looking for re-election.\\xe2\\x80\\x9d The probability of inserting this word is based on the syntactic structure of the node into which it\\xe2\\x80\\x99s in- serted.  Knight and Marcu (2000) describe in detail a noisy-channel model that explains how short sen- tences can be expanded into longer ones by inserting and expanding syntactic constituents (and words). Since our constituent-expand stochastic operation simply reimplements Knight and Marcu\\xe2\\x80\\x99s model  we do not focus on them here. We refer the reader to (Knight and Marcu  2000) for the details.  In addition to adding syntactic constituents  our system is also able to add discourse units. Consider the summary \\xef\\xbf\\xbd\\\\\\xef\\xbf\\xbd \\xe2\\x80\\x9cJohn Doe has already secured the vote of most democrats in his consituency.\\xe2\\x80\\x9d Through a sequence of discourse expansions  we can expand upon this summary to reach the original text. A com- plete discourse expansion process that would occur starting from this initial summary to generate the original document is shown in Figure 2.  In this figure  we can follow the sequence of steps required to generate our original text  begin- ning with our summary \\xef\\xbf\\xbd . First  through an op- eration D-Project (\\xe2\\x80\\x9cD\\xe2\\x80\\x9d for \\xe2\\x80\\x9cD\\xe2\\x80\\x9discourse)  we in- crease the depth of the tree  adding an intermediate  NUC=SPAN node. This projection adds a factor of \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd Nuc=Span ] Nuc=Span \\xef\\xbf\\xbd Nuc=Span \" to the probabil- ity of this sequence of operations (as is shown under the arrow).  We are now able to perform the second operation  D-Expand  with which we expand on the core mes- sage contained in \\xef\\xbf\\xbd by adding a satellite which eval- uates the information presented in \\xef\\xbf\\xbd . This expansion adds the probability of performing the expansion (called the discourse expansion probabilities  \\xef\\xbf\\xbd<BC^ . An example discourse expansion probability  writ- ten \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd Nuc=Span ] Nuc=Span Sat=Eval \\xef\\xbf\\xbd Nuc=Span ] Nuc=Span \"   reflects the probability of adding an eval- uation satellite onto a nuclear span).  The rest of Figure 2 shows some of the remaining steps to produce the original document  each step la- beled with the appropriate probability factors. Then  the probability of the entire expansion is the prod- uct of all those listed probabilities combined with the appropriate probabilities from the syntax side of things. In order to produce the final score \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd$\\xef\\xbf\\xbd%\\xef\\xbf\\xbd&\" for a document/summary pair  we multiply together each of the expansion probabilities in the path lead- ing from \\xef\\xbf\\xbd to \\xef\\xbf\\xbd .  For estimating the parameters for the discourse models  we used an RST corpus of 385 Wall Street Journal articles from the Penn Treebank  which we obtained from LDC. The documents in the corpus range in size from 31 to 2124 words  with an av- erage of 458 words per document. Each document is paired with a discourse structure that was manu-    _ ` ` a  John Doe has already secured the vote of most democrats in his constituency   b c d e f g h i which is already almost enough to win.  f h a e j k h l c h a m ` i b c d e f g h i n o p q r s t u vn o w x y z { |n o p q r s t u v  _ ` ` a John Doe has already secured the vote of most democrats in his constituency   b c d e f g h i  } ~\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd  _ ` ` a  John Doe has already secured the vote of most democrats in his constituency   b c d e f g h i b c d e f g h i  John Doe has already secured the vote of most democrats in his constituency   b c d e f g h i which is already almost enough to win.  f h a e j k h l c h a m ` i But without the support of the governer   f h a e d ` i \\xef\\xbf\\xbd m a m ` i he is still on shaky ground.  b c d e f g h i  b c d e f g h i b c d e \\xef\\xbf\\xbd ` i a \\xef\\xbf\\xbd h \\xef\\xbf\\xbd ab c d e \\xef\\xbf\\xbd ` i a \\xef\\xbf\\xbd h \\xef\\xbf\\xbd a  _ ` ` a n o w x y z { |  John Doe has already secured the vote of most democrats in his constituency   b c d e f g h i which is already almost enough to win.  f h a e j k h l c h a m ` i But without the support of the governer   f h a e d ` i \\xef\\xbf\\xbd m a m ` i he is still on shaky ground.  b c d e f g h i The mayor is  now looking for re\\xe2\\x88\\x92election.  f h a e \\xef\\xbf\\xbd h d \\xef\\xbf\\xbd \\xef\\xbf\\xbd \\xef\\xbf\\xbd ` c i \\xef\\xbf\\xbd b c d e f g h i b c d e \\xef\\xbf\\xbd ` i a \\xef\\xbf\\xbd h \\xef\\xbf\\xbd ab c d e \\xef\\xbf\\xbd ` i a \\xef\\xbf\\xbd h \\xef\\xbf\\xbd a  _ ` ` a  _ ` ` a  John Doe has already secured the vote of most democrats in his constituency   b c d e f g h i which is already almost enough to win.  f h a e j k h l c h a m ` i b c d e \\xef\\xbf\\xbd ` i a \\xef\\xbf\\xbd h \\xef\\xbf\\xbd a b c d e f g h i  John Doe has already secured the vote of most democrats in his constituency   b c d e f g h i which is already almost enough to win.  f h a e j k h l c h a m ` i  b c d e f g h i b c d e \\xef\\xbf\\xbd ` i a \\xef\\xbf\\xbd h \\xef\\xbf\\xbd a  _ ` ` a  he is still on shaky ground.  b c d e \\xef\\xbf\\xbd ` i a \\xef\\xbf\\xbd h \\xef\\xbf\\xbd a  P(Nuc=Span \\xe2\\x88\\x92> Nuc=Span                          Sat=evaluation    Nuc=Span \\xe2\\x88\\x92> Nuc=Span)  P(Nuc=Span \\xe2\\x88\\x92> Nuc=Span |  P(Nuc=Span \\xe2\\x88\\x92> Nuc=Contrast                              Nuc=Contrast |  P(Root \\xe2\\x88\\x92> Sat=Background Nuc=Span |                 Root \\xe2\\x88\\x92> Nuc=Span)  Nuc=Span)  P(Nuc=Span \\xe2\\x88\\x92> Nuc=Contrast | Nuc=Span)       Nuc=Span \\xe2\\x88\\x92> Nuc=Contrast)  P(Nuc=Contrast \\xe2\\x88\\x92> Sat=condiation Nuc=Span |                  Nuc=Contrast \\xe2\\x88\\x92> Nuc=Span)n o w x y z { |n o p q r s t u v P(Nuc=Contrast \\xe2\\x88\\x92> Nuc=Span | Nuc=Contrast)*  Figure 2: A sequence of discourse expansions for Text (1) (with probability factors).  ally built in the style of RST. (See (Carlson et al.  2001) for details concerning the corpus and the an- notation process.) From this corpus  we were able to estimate parameters for a discourse PCFG using standard maximum likelihood methods.  Furthermore  150 document from the same corpus are paired with extractive summaries on the EDU level. Human annotators were asked which EDUs were most important; suppose in the example DS- tree (Figure 1) the annotators marked the second and fifth EDUs (the starred ones). These stars are propagated up  so that any discourse unit that has a descendent considered important is also consid- ered important. From these annotations  we could deduce that  to compress a NUC=CONTRAST that has two children  NUC=SPAN and SAT=EVALUATION  we can drop the evaluation satellite. Similarly  we can compress a NUC=CONTRAST that has two children  SAT=CONDITION and NUC=SPAN by dropping the first discourse constituent. Finally  we can compress the ROOT deriving into SAT=BACKGROUND NUC=SPAN by dropping the SAT=BACKGROUND constituent. We keep counts of each of these examples and  once col- lected  we normalize them to get the discourse ex- pansion probabilities.  3.3 Decoder  The goal of the decoder is to combine \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd)\\xef\\xbf\\xbd&\" with \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd$\\xef\\xbf\\xbd%\\xef\\xbf\\xbd&\" to get \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd%\\xef\\xbf\\xbd \" . There are a vast number of potential compressions of a large DS-tree  but  we can efficiently pack them into a shared-forest structure  as described in detail by Knight & Marcu (2000). Each entry in the shared-forest structure has three associated probabilities  one from the source syntax PCFG  one from the source discourse PCFG and one from the expansion-template probabilities described in Section 3.2. Once we have generated a forest representing all possible compressions of the original document  we want to extract the best (or the \\xef\\xbf\\xbd -best) trees  taking into account both the ex- pansion probabilities of the channel model and the bigram and syntax and discourse PCFG probabili- ties of the source model. Thankfully  such a generic extractor has already been built (Langkilde  2000). For our purposes  the extractor selects the trees with the best combination of LM and expansion scores after performing an exhaustive search over all possi- ble summaries. It returns a list of such trees  one for each possible length.  4 System  The system developed works in a pipelined fash- ion as shown in Figure 3. The first step along the pipeline is to generate the discourse structure. To do this  we use the decision-based discourse parser described by Marcu (2000)2. Once we have the dis- course structure  we send each EDU off to a syn-  2The discourse parser achieves an f-score of \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd for EDU identification  \\xef\\xbf\\xbd\\r\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd for identifying hierarchical spans  \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd for nuclearity identification and \\xef\\xbf\\xbd\\r\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd for relation tagging.    Parser Discourse Syntax  Parser Forest Generator  Decoder Chooser Length  Output Summary  Input Document  Figure 3: The pipeline of system components.  tactic parser (Collins  1997). The syntax trees of the EDUs are then merged with the discourse tree in the forest generator to create a DS-tree similar to that shown in Figure 1. From this DS-tree we gener- ate a forest that subsumes all possible compressions. This forest is then passed on to the forest ranking system which is used as decoder (Langkilde  2000). The decoder gives us a list of possible compressions  for each possible length. Example compressions of Text (1) are shown in Figure 4 together with their respective log-probabilities.  In order to choose the \\xe2\\x80\\x9cbest\\xe2\\x80\\x9d compression at any possible length  we cannot rely only on the log-probabilities  lest the system always choose the shortest possible compression. In order to compen- sate for this  we normalize by length. However  in practice  simply dividing the log-probability by the length of the compression is insufficient for longer documents. Experimentally  we found a reasonable metric was to  for a compression of length \\xef\\xbf\\xbd   divide each log-probability by \\xef\\xbf\\xbd \\xef\\xbf\\xbd\\'\\xef\\xbf\\xbd \\xef\\xbf\\xbd . This was the job of the length chooser from Figure 3  and enabled us to choose a single compression for each document  which was used for evaluation. (In Figure 4  the compression chosen by the length selector is itali- cized and was the shortest one3.)  5 Results  For testing  we began with two sets of data. The first set is drawn from the Wall Street Journal (WSJ) portion of the Penn Treebank and consists of \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd doc- uments  each containing between \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd and \\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd words. The second set is drawn from a collection of stu-  3This tends to be the case for very short documents  as the compressions never get sufficiently long for the length normal- ization to have an effect.  dent compositions and consists of \\xef\\xbf\\xbd documents  each containing between \\xef\\xbf\\xbd.\\xef\\xbf\\xbd and \\xef\\xbf\\xbd\\xc2\\xa0\\xef\\xbf\\xbd words. We call this set the MITRE corpus (Hirschman et al.  1999). We would liked to have run evaluations on longer docu- ments. Unfortunately  the forests generated even for relatively small documents are huge. Because there are an exponential number of summaries that can be generated for any given text4  the decoder runs out of memory for longer documents; therefore  we se- lected shorter subtexts from the original documents.  We used both the WSJ and Mitre data for eval- uation because we wanted to see whether the per- formance of our system varies with text genre. The Mitre data consists mostly of short sentences (av- erage document length from Mitre is \\xef\\xbf\\xbd sentences)  quite in constrast to the typically long sentences in the Wall Street Journal articles (average document length from WSJ is \\xc2\\xa1\\xc2\\xa0\\xef\\xbf\\xbd4\\xc2\\xa2\\xef\\xbf\\xbd\\xef\\xbf\\xbd sentences).  For purpose of comparison  the Mitre data was compressed using five systems:  Random: Drops random words (each word has a 50% chance of being dropped (baseline).  Hand: Hand compressions done by a human.  Concat: Each sentence is compressed individually; the results are concatenated together  using Knight & Marcu\\xe2\\x80\\x99s (2000) system here for com- parison.  EDU: The system described in this paper.  Sent: Because syntactic parsers tend not to work well parsing just clauses  this system merges together leaves in the discourse tree which are in the same sentence  and then proceeds as de- scribed in this paper.  The Wall Street Journal data was evaluated on the above five systems as well as two additions. Since the correct discourse trees were known for these data  we thought it wise to test the systems using these human-built discourse trees  instead of the au- tomatically derived ones. The additionall two sys- tems were:  PD-EDU: Same as EDU except using the perfect discourse trees  available from the RST corpus (Carlson et al.  2001).  4In theory  a text of \\xc2\\xa3 words has \\xef\\xbf\\xbd6\\xc2\\xa4 possible compressions.    len log prob best compression\\xef\\xbf\\xbd \\xc2\\xa5C\\xc2\\xa6\\xef\\xbf\\xbd\\xc2\\xa6\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd6\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xc2\\xa76\\xef\\xbf\\xbd Mayor is now looking which is enough.\\xc2\\xa6\\xc2\\xa8\\xef\\xbf\\xbd \\xc2\\xa5C\\xc2\\xa6\\xc2\\xa8\\xef\\xbf\\xbd\\xc2\\xaa\\xc2\\xa9\\xc2\\xab\\xef\\xbf\\xbd4\\xc2\\xa6\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xc2\\xa6\\xef\\xbf\\xbd\\xef\\xbf\\xbd The mayor is now looking which is already almost enough to win.\\xc2\\xa6\\xc2\\xa8\\xc2\\xa7 \\xc2\\xa5C\\xc2\\xa6\\xc2\\xa8\\xef\\xbf\\xbd\\xc2\\xaa\\xc2\\xa9\\xc2\\xab\\xef\\xbf\\xbd \\xef\\xbf\\xbd\\'\\xef\\xbf\\xbd\\xc2\\xac\\xc2\\xa9\\'\\xef\\xbf\\xbd The mayor is now looking but without support  he is still on shaky ground.\\xc2\\xa6\\xc2\\xa8\\xef\\xbf\\xbd \\xc2\\xa5C\\xc2\\xa6\\xc2\\xa8\\xc2\\xa76\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd6\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xc2\\xa6\\xef\\xbf\\xbd\\xef\\xbf\\xbd Mayor is now looking but without the support of governer  he is still on shaky ground.\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xc2\\xa5C\\xc2\\xa68\\xc2\\xa9\\'\\xc2\\xa7\\xef\\xbf\\xbd\\xef\\xbf\\xbd4\\xc2\\xa6\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd6\\xef\\xbf\\xbd The mayor is now looking for re-election but without the support of the governer  he is still on shaky ground.\\xef\\xbf\\xbd\\r\\xef\\xbf\\xbd \\xc2\\xa5\\xc2\\xad\\xef\\xbf\\xbd\\r\\xef\\xbf\\xbd6\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd \\xef\\xbf\\xbd6\\xef\\xbf\\xbd\\xef\\xbf\\xbd\\xef\\xbf\\xbd6\\xef\\xbf\\xbd The mayor is now looking which is already almost enough to win. But without the support of the governer  he is still on shaky ground.  Figure 4: Possible compressions for Text (1).  PD-Sent: The same as Sent except using the perfect discourse trees.  Six human evaluators rated the systems according to three metrics. The first two  presented together to the evaluators  were grammaticality and coherence; the third  presented separately  was summary qual- ity. Grammaticality was a judgment of how good the English of the compressions were; coherence included how well the compression flowed (for in- stance  anaphors lacking an antecedent would lower coherence). Summary quality  on the other hand  was a judgment of how well the compression re- tained the meaning of the original document. Each measure was rated on a scale from \\xef\\xbf\\xbd (worst) to \\xef\\xbf\\xbd (best).  We can draw several conclusions from the eval- uation results shown in Table 2 along with aver- age compression rate (Cmp  the length of the com- pressed document divided by the original length).5  First  it is clear that genre influences the results. Because the Mitre data contained mostly short sen- tences  the syntax and discourse parsers made fewer errors  which allowed for better compressions to be generated. For the Mitre corpus  compressions ob- tained starting from discourse trees built above the sentence level were better than compressions ob- tained starting from discourse trees built above the EDU level. For the WSJ corpus  compression ob- tained starting from discourse trees built above the sentence level were more grammatical  but less co- herent than compressions obtained starting from dis- course trees built above the EDU level. Choosing the manner in which the discourse and syntactic repre- sentations of texts are mixed should be influenced by the genre of the texts one is interested to compress.  5We did not run the system on the MITRE data with perfect discourse trees because we did not have hand-built discourse trees for this corpus.  WSJ Mitre Cmp Grm Coh Qual Cmp Grm Coh Qual  Random 0.51 1.60 1.58 2.13 0.47 1.43 1.77 1.80 Concat 0.44 3.30 2.98 2.70 0.42 2.87 2.50 2.08  EDU 0.49 3.36 3.33 3.03 0.47 3.40 3.30 2.60 Sent 0.47 3.45 3.16 2.88 0.44 4.27 3.63 3.36  PD-EDU 0.47 3.61 3.23 2.95 PD-Sent 0.48 3.96 3.65 2.84  Hand 0.59 4.65 4.48 4.53 0.46 4.97 4.80 4.52  Table 2: Evaluation Results  The compressions obtained starting from per- fectly derived discourse trees indicate that perfect discourse structures help greatly in improving coher- ence and grammaticality of generated summaries. It was surprising to see that the summary quality was affected negatively by the use of perfect discourse structures (although not statistically significant). We believe this happened because the text fragments we summarized were extracted from longer documents. It is likely that had the discourse structures been built specifically for these short text snippets  they would have been different. Moreover  there was no compo- nent designed to handle cohesion; t\n"
          ]
        }
      ],
      "source": [
        "print(X['Content'][1308]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74e9be5b",
      "metadata": {
        "id": "74e9be5b",
        "outputId": "b37947b7-7f5a-4463-d5b2-3d3ae7121b49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A Noisy-Channel Model for Document Compression\n",
            "2002.0\n"
          ]
        }
      ],
      "source": [
        "print(X['Title'][1308]) \n",
        "print(X['Year of Publish'][1308]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5818f10c",
      "metadata": {
        "id": "5818f10c",
        "outputId": "eaa02385-6461-4556-d21c-60130571adae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "search : 1308 - title = A Noisy-Channel Model for Document Compression\n",
            "\n",
            "Best Results :\n",
            "id =  2968 - title = Text-level Discourse Parsing with Rich Linguistic Features\n",
            "id =  3090 - title = A Two-step Approach to Sentence Compression of Spoken Utterances\n",
            "id =  2205 - title = Mining Wikipedia Revision Histories for Improving Sentence Compression\n",
            "id =  4004 - title = Learning Representations for Text-level Discourse Parsing\n",
            "id =  4404 - title = Joint Modeling of Content and Discourse Relations in Dialogues\n"
          ]
        }
      ],
      "source": [
        "index = 1308 \n",
        "result = find_similar(tfidf, index, top_n = 5) \n",
        "print_result('1308 - title = A Noisy-Channel Model for Document Compression', result, X)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}